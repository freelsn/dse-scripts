{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run ml_header.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run get_data.py\n",
    "benchmarks = ('adpcm_encoder', 'aes', 'ann', 'average', 'decimation', 'fft_fixed', 'fir',\n",
    "              'idct', 'interpolation', 'kasumi', 'qsort', 'snow3g', 'sobel')  # 13\n",
    "gd = GetData('data/ASIC-2-FPGA', benchmarks)\n",
    "gd.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run methods.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run machine_learning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('adpcm_encoder',\n",
       "  'aes',\n",
       "  'ann',\n",
       "  'average',\n",
       "  'decimation',\n",
       "  'fft_fixed',\n",
       "  'fir',\n",
       "  'idct',\n",
       "  'interpolation',\n",
       "  'kasumi',\n",
       "  'qsort',\n",
       "  'snow3g'),\n",
       " ('adpcm_encoder',\n",
       "  'aes',\n",
       "  'ann',\n",
       "  'average',\n",
       "  'decimation',\n",
       "  'fft_fixed',\n",
       "  'fir',\n",
       "  'idct',\n",
       "  'interpolation',\n",
       "  'kasumi',\n",
       "  'qsort',\n",
       "  'sobel'),\n",
       " ('adpcm_encoder',\n",
       "  'aes',\n",
       "  'ann',\n",
       "  'average',\n",
       "  'decimation',\n",
       "  'fft_fixed',\n",
       "  'fir',\n",
       "  'idct',\n",
       "  'interpolation',\n",
       "  'kasumi',\n",
       "  'snow3g',\n",
       "  'sobel'),\n",
       " ('adpcm_encoder',\n",
       "  'aes',\n",
       "  'ann',\n",
       "  'average',\n",
       "  'decimation',\n",
       "  'fft_fixed',\n",
       "  'fir',\n",
       "  'idct',\n",
       "  'interpolation',\n",
       "  'qsort',\n",
       "  'snow3g',\n",
       "  'sobel'),\n",
       " ('adpcm_encoder',\n",
       "  'aes',\n",
       "  'ann',\n",
       "  'average',\n",
       "  'decimation',\n",
       "  'fft_fixed',\n",
       "  'fir',\n",
       "  'idct',\n",
       "  'kasumi',\n",
       "  'qsort',\n",
       "  'snow3g',\n",
       "  'sobel'),\n",
       " ('adpcm_encoder',\n",
       "  'aes',\n",
       "  'ann',\n",
       "  'average',\n",
       "  'decimation',\n",
       "  'fft_fixed',\n",
       "  'fir',\n",
       "  'interpolation',\n",
       "  'kasumi',\n",
       "  'qsort',\n",
       "  'snow3g',\n",
       "  'sobel'),\n",
       " ('adpcm_encoder',\n",
       "  'aes',\n",
       "  'ann',\n",
       "  'average',\n",
       "  'decimation',\n",
       "  'fft_fixed',\n",
       "  'idct',\n",
       "  'interpolation',\n",
       "  'kasumi',\n",
       "  'qsort',\n",
       "  'snow3g',\n",
       "  'sobel'),\n",
       " ('adpcm_encoder',\n",
       "  'aes',\n",
       "  'ann',\n",
       "  'average',\n",
       "  'decimation',\n",
       "  'fir',\n",
       "  'idct',\n",
       "  'interpolation',\n",
       "  'kasumi',\n",
       "  'qsort',\n",
       "  'snow3g',\n",
       "  'sobel'),\n",
       " ('adpcm_encoder',\n",
       "  'aes',\n",
       "  'ann',\n",
       "  'average',\n",
       "  'fft_fixed',\n",
       "  'fir',\n",
       "  'idct',\n",
       "  'interpolation',\n",
       "  'kasumi',\n",
       "  'qsort',\n",
       "  'snow3g',\n",
       "  'sobel'),\n",
       " ('adpcm_encoder',\n",
       "  'aes',\n",
       "  'ann',\n",
       "  'decimation',\n",
       "  'fft_fixed',\n",
       "  'fir',\n",
       "  'idct',\n",
       "  'interpolation',\n",
       "  'kasumi',\n",
       "  'qsort',\n",
       "  'snow3g',\n",
       "  'sobel'),\n",
       " ('adpcm_encoder',\n",
       "  'aes',\n",
       "  'average',\n",
       "  'decimation',\n",
       "  'fft_fixed',\n",
       "  'fir',\n",
       "  'idct',\n",
       "  'interpolation',\n",
       "  'kasumi',\n",
       "  'qsort',\n",
       "  'snow3g',\n",
       "  'sobel'),\n",
       " ('adpcm_encoder',\n",
       "  'ann',\n",
       "  'average',\n",
       "  'decimation',\n",
       "  'fft_fixed',\n",
       "  'fir',\n",
       "  'idct',\n",
       "  'interpolation',\n",
       "  'kasumi',\n",
       "  'qsort',\n",
       "  'snow3g',\n",
       "  'sobel'),\n",
       " ('aes',\n",
       "  'ann',\n",
       "  'average',\n",
       "  'decimation',\n",
       "  'fft_fixed',\n",
       "  'fir',\n",
       "  'idct',\n",
       "  'interpolation',\n",
       "  'kasumi',\n",
       "  'qsort',\n",
       "  'snow3g',\n",
       "  'sobel')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.combinations(benchmarks, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['AREA', 'state', 'FU', 'REG', 'MUX', 'DEC', 'pin_pair',\n",
    "            'net', 'max', 'min', 'ave', 'MISC', 'MEM', 'sim', 'Pmax',\n",
    "            'Pmin', 'Pave', 'Latency', 'BlockMemoryBit', 'DSP', 'Slices']\n",
    "invalid_features = ['Slices', 'Latency']\n",
    "valid_features = [i for i in features if i not in invalid_features]\n",
    "label = 'Slices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def execute(df, estimator, kind):\n",
    "    # remove attribute columns\n",
    "    data = df[features].copy()\n",
    "    # fix missing data\n",
    "    data = ML.fix_missing_data(data)\n",
    "    # X, y\n",
    "    X, y = ML.separate_feature_label(data, valid_features=valid_features, label=label)\n",
    "    # feature scaling\n",
    "    X = ML.feature_scaling(X)\n",
    "    if kind == 'training':\n",
    "        estimator.fit(X, y)\n",
    "        return estimator\n",
    "    elif kind == 'testing':\n",
    "        y_pred = estimator.predict(X)\n",
    "        data['AREA'] = y_pred\n",
    "        return DirectMapping.main(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adrs_ave': 0.0,\n",
       " 'adrs_ave_rms': 0.0,\n",
       " 'adrs_max': 0.0,\n",
       " 'adrs_max_rms': 0.0,\n",
       " 'cardinality': 3,\n",
       " 'dominance': 1.0,\n",
       " 'hypervolume': 0.43148731064332085}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = execute(gd.data_v4['adpcm_encoder'], estimators[0], kind='training')\n",
    "scores = execute(gd.data_v4['average'], estimator, kind='testing')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  123.3893,   -19.6479,   120.7541,    18.8534,    -5.8628,\n",
       "          -0.    ,  1292.9906, -1136.0835,   -19.6479,   -19.6479,\n",
       "         -19.6479,    83.4866,     0.    ,     0.    ,     0.    ,\n",
       "           0.    ,     0.    ,     0.    ,     0.    ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_1 = dict()\n",
    "for benchmarks_train in itertools.combinations(benchmarks, 1):\n",
    "    benchmarks_test = (i for i in benchmarks if i not in benchmarks_train)\n",
    "    data_train = pd.concat([gd.data_v4[i] for i in benchmarks_train], axis=0, ignore_index=True)\n",
    "    estimator = execute(data_train, estimators[0], kind='training')\n",
    "    key = ';'.join(list(benchmarks_train))\n",
    "    scores_1[key] = dict()\n",
    "    for benchmark_test in benchmarks_test:\n",
    "        scores_1[key][benchmark_test] = execute(gd.data_v4[benchmark_test], estimator, kind='testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_2 = dict()\n",
    "for benchmarks_train in itertools.combinations(benchmarks, 2):\n",
    "    benchmarks_test = (i for i in benchmarks if i not in benchmarks_train)\n",
    "    data_train = pd.concat([gd.data_v4[i] for i in benchmarks_train], axis=0, ignore_index=True)\n",
    "    estimator = execute(data_train, estimators[0], kind='training')\n",
    "    key = ';'.join(list(benchmarks_train))\n",
    "    scores_2[key] = dict()\n",
    "    for benchmark_test in benchmarks_test:\n",
    "        scores_2[key][benchmark_test] = execute(gd.data_v4[benchmark_test], estimator, kind='testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 3min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_v4 = dict()\n",
    "for benchmarks_cnt in range(1, len(benchmarks)):\n",
    "    scores = dict()\n",
    "    for benchmarks_train in itertools.combinations(benchmarks, benchmarks_cnt):\n",
    "        benchmarks_test = (i for i in benchmarks if i not in benchmarks_train)\n",
    "        data_train = pd.concat([gd.data_v4[i] for i in benchmarks_train], axis=0, ignore_index=True)\n",
    "        estimator = execute(data_train, estimators[0], kind='training')\n",
    "        key = ';'.join(list(benchmarks_train))\n",
    "        scores[key] = dict()\n",
    "        for benchmark_test in benchmarks_test:\n",
    "            scores[key][benchmark_test] = execute(gd.data_v4[benchmark_test], estimator, kind='testing')\n",
    "    scores_v4[str(benchmarks_cnt)] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 6min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_v5 = dict()\n",
    "for benchmarks_cnt in range(1, len(benchmarks)):\n",
    "    scores = dict()\n",
    "    for benchmarks_train in itertools.combinations(benchmarks, benchmarks_cnt):\n",
    "        benchmarks_test = (i for i in benchmarks if i not in benchmarks_train)\n",
    "        data_train = pd.concat([gd.data_v5[i] for i in benchmarks_train], axis=0, ignore_index=True)\n",
    "        estimator = execute(data_train, estimators[0], kind='training')\n",
    "        key = ';'.join(list(benchmarks_train))\n",
    "        scores[key] = dict()\n",
    "        for benchmark_test in benchmarks_test:\n",
    "            scores[key][benchmark_test] = execute(gd.data_v5[benchmark_test], estimator, kind='testing')\n",
    "    scores_v5[str(benchmarks_cnt)] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('results_v4.csv', 'w') as f:\n",
    "    f.write('train,test,adrs_ave,adrs_max,adrs_ave_rms,adrs_max_rms,hypervolume,dominance,cardinality\\n')\n",
    "    for bench_cnt in scores_v4_rf.keys():\n",
    "        for training, testing in scores_v4_rf[bench_cnt].items():\n",
    "            for k, v in testing.items():\n",
    "                f.write(training + ',' + k + ',' + ','.join([f'{i:.4f}' for i in v.values()]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('results_v5.csv', 'w') as f:\n",
    "    f.write('train,test,adrs_ave,adrs_max,adrs_ave_rms,adrs_max_rms,hypervolume,dominance,cardinality\\n')\n",
    "    for bench_cnt in scores_v5_ada.keys():\n",
    "        for training, testing in scores_v5[bench_cnt].items():\n",
    "            for k, v in testing.items():\n",
    "                f.write(training + ',' + k + ',' + ','.join([f'{i:.4f}' for i in v.values()]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best'),\n",
       "         learning_rate=1.0, loss='linear', n_estimators=300,\n",
       "         random_state=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17h 9min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_v4_ada = dict()\n",
    "for benchmarks_cnt in range(1, len(benchmarks)):\n",
    "    scores = dict()\n",
    "    for benchmarks_train in itertools.combinations(benchmarks, benchmarks_cnt):\n",
    "        benchmarks_test = (i for i in benchmarks if i not in benchmarks_train)\n",
    "        data_train = pd.concat([gd.data_v4[i] for i in benchmarks_train], axis=0, ignore_index=True)\n",
    "        estimator = execute(data_train, estimators[-2], kind='training')\n",
    "        key = ';'.join(list(benchmarks_train))\n",
    "        scores[key] = dict()\n",
    "        for benchmark_test in benchmarks_test:\n",
    "            scores[key][benchmark_test] = execute(gd.data_v4[benchmark_test], estimator, kind='testing')\n",
    "    scores_v4_ada[str(benchmarks_cnt)] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17h 7min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_v5_ada = dict()\n",
    "for benchmarks_cnt in range(1, len(benchmarks)):\n",
    "    scores = dict()\n",
    "    for benchmarks_train in itertools.combinations(benchmarks, benchmarks_cnt):\n",
    "        benchmarks_test = (i for i in benchmarks if i not in benchmarks_train)\n",
    "        data_train = pd.concat([gd.data_v5[i] for i in benchmarks_train], axis=0, ignore_index=True)\n",
    "        estimator = execute(data_train, estimators[-2], kind='training')\n",
    "        key = ';'.join(list(benchmarks_train))\n",
    "        scores[key] = dict()\n",
    "        for benchmark_test in benchmarks_test:\n",
    "            scores[key][benchmark_test] = execute(gd.data_v4[benchmark_test], estimator, kind='testing')\n",
    "    scores_v5_ada[str(benchmarks_cnt)] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = estimators[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16h 55min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_v4_rf = dict()\n",
    "for benchmarks_cnt in range(1, len(benchmarks)):\n",
    "    scores = dict()\n",
    "    for benchmarks_train in itertools.combinations(benchmarks, benchmarks_cnt):\n",
    "        benchmarks_test = (i for i in benchmarks if i not in benchmarks_train)\n",
    "        data_train = pd.concat([gd.data_v4[i] for i in benchmarks_train], axis=0, ignore_index=True)\n",
    "        estimator = execute(data_train, estimators[-2], kind='training')\n",
    "        key = ';'.join(list(benchmarks_train))\n",
    "        scores[key] = dict()\n",
    "        for benchmark_test in benchmarks_test:\n",
    "            scores[key][benchmark_test] = execute(gd.data_v4[benchmark_test], estimator, kind='testing')\n",
    "    scores_v4_rf[str(benchmarks_cnt)] = scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
